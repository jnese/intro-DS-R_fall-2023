{
  "hash": "da26cc6622245a4109fd2b787ec9adc7",
  "result": {
    "markdown": "---\ntitle: \"Homework 4\"\nauthor: \"Key\"\nformat:\n  html:\n    code-fold: true\nexecute: \n  error: true\n  message: false\n  warning: false\n---\n\n\n# Create a RStudio Project on your machine\n\n-   Name the Project \"hw_4\"\n-   Within the Project create a folder for *data* and *scripts*.\n-   Download the following three datasets into the project *data* folder.\n    -   *sesame13.sav*\n    -   *star.csv*\n    -   *ais.xlsx*\n-   Save a hw_4.Rmd file in your *scripts* folder\n\n# Part 1\n\nInstall and load the package `{Lahman}`, which will give you access to the *Teams* data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"Lahman\")\nlibrary(rio)\nlibrary(here)\nlibrary(tidyverse)\nlibrary(Lahman)\nlibrary(janitor)\n```\n:::\n\n\n#### 1. Produce a subset of the data (as a new object) that has the following characteristics:\n\n-   only one team (your choice)\n-   (try to you select a team that is currently still around, or it probably won't be interesting; see a list of current at <http://www.espn.com/mlb/teams>).\n-   data from 1980 to present (or as current as the dataset allows)\n-   includes 5 columns: `name`, `yearID`, `W`, `L`, `R`, `RA`. These 5 variables correspond to the team name, the year, wins, losses, runs scored, and runs allowed).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nteams <- Teams %>% \n  janitor::clean_names()\n\ncubs <- teams %>% \n    filter(name == \"Chicago Cubs\" & year_id >= 1980) %>% # Only one team (your choice) & data from 1980 to present\n    select(name, year_id, w, l, r, ra) #Includes 5 columns: name, yearID, W, L, R, RA\n\n\n# Create a new variable corresponding to the winning percentage for the team you chose over time\ncubs <- cubs %>% \n    mutate(w_pct = w / (w + l))\n```\n:::\n\n\n#### 2. Create a new variable corresponding to the winning percentage for the team you chose over time:\n\n$$w_{pct} = \\frac{wins}{wins + losses}$$ \\* Order by winning percentage: Least to greatest \\* Order by winning percentage: Greatest to least \\* Compute the `mean` and `standard deviation` of winning percentage\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Order by winning percentage: Least to greatest\ncubs %>% \n  arrange(w_pct)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           name year_id   w   l   r  ra     w_pct\n1  Chicago Cubs    1981  38  65 370 483 0.3689320\n2  Chicago Cubs    2012  61 101 613 759 0.3765432\n3  Chicago Cubs    1980  64  98 614 728 0.3950617\n4  Chicago Cubs    2000  65  97 764 904 0.4012346\n5  Chicago Cubs    2006  66  96 716 834 0.4074074\n6  Chicago Cubs    2013  66  96 602 689 0.4074074\n7  Chicago Cubs    1999  67  95 747 920 0.4135802\n8  Chicago Cubs    2002  67  95 706 759 0.4135802\n9  Chicago Cubs    1997  68  94 687 759 0.4197531\n10 Chicago Cubs    1994  49  64 500 549 0.4336283\n11 Chicago Cubs    1986  70  90 680 781 0.4375000\n12 Chicago Cubs    1983  71  91 701 719 0.4382716\n13 Chicago Cubs    2011  71  91 654 756 0.4382716\n14 Chicago Cubs    2021  71  91 705 839 0.4382716\n15 Chicago Cubs    1982  73  89 676 709 0.4506173\n16 Chicago Cubs    2014  73  89 614 707 0.4506173\n17 Chicago Cubs    2022  74  88 657 731 0.4567901\n18 Chicago Cubs    2010  75  87 685 767 0.4629630\n19 Chicago Cubs    1996  76  86 772 771 0.4691358\n20 Chicago Cubs    1987  76  85 720 801 0.4720497\n21 Chicago Cubs    1988  77  85 660 694 0.4753086\n22 Chicago Cubs    1990  77  85 690 774 0.4753086\n23 Chicago Cubs    1985  77  84 686 729 0.4782609\n24 Chicago Cubs    1991  77  83 695 734 0.4812500\n25 Chicago Cubs    1992  78  84 593 624 0.4814815\n26 Chicago Cubs    2005  79  83 703 714 0.4876543\n27 Chicago Cubs    1995  73  71 693 671 0.5069444\n28 Chicago Cubs    2009  83  78 707 672 0.5155280\n29 Chicago Cubs    1993  84  78 738 739 0.5185185\n30 Chicago Cubs    2019  84  78 814 717 0.5185185\n31 Chicago Cubs    2007  85  77 752 690 0.5246914\n32 Chicago Cubs    2001  88  74 777 701 0.5432099\n33 Chicago Cubs    2003  88  74 724 683 0.5432099\n34 Chicago Cubs    2004  89  73 789 665 0.5493827\n35 Chicago Cubs    1998  90  73 831 792 0.5521472\n36 Chicago Cubs    2020  34  26 265 240 0.5666667\n37 Chicago Cubs    2017  92  70 822 695 0.5679012\n38 Chicago Cubs    1989  93  69 702 623 0.5740741\n39 Chicago Cubs    2018  95  68 761 645 0.5828221\n40 Chicago Cubs    1984  96  65 762 658 0.5962733\n41 Chicago Cubs    2015  97  65 689 608 0.5987654\n42 Chicago Cubs    2008  97  64 855 671 0.6024845\n43 Chicago Cubs    2016 103  58 808 556 0.6397516\n```\n:::\n\n```{.r .cell-code}\n# Order by winning percentage: greatest to least\ncubs %>% \n  arrange(desc(w_pct))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           name year_id   w   l   r  ra     w_pct\n1  Chicago Cubs    2016 103  58 808 556 0.6397516\n2  Chicago Cubs    2008  97  64 855 671 0.6024845\n3  Chicago Cubs    2015  97  65 689 608 0.5987654\n4  Chicago Cubs    1984  96  65 762 658 0.5962733\n5  Chicago Cubs    2018  95  68 761 645 0.5828221\n6  Chicago Cubs    1989  93  69 702 623 0.5740741\n7  Chicago Cubs    2017  92  70 822 695 0.5679012\n8  Chicago Cubs    2020  34  26 265 240 0.5666667\n9  Chicago Cubs    1998  90  73 831 792 0.5521472\n10 Chicago Cubs    2004  89  73 789 665 0.5493827\n11 Chicago Cubs    2001  88  74 777 701 0.5432099\n12 Chicago Cubs    2003  88  74 724 683 0.5432099\n13 Chicago Cubs    2007  85  77 752 690 0.5246914\n14 Chicago Cubs    1993  84  78 738 739 0.5185185\n15 Chicago Cubs    2019  84  78 814 717 0.5185185\n16 Chicago Cubs    2009  83  78 707 672 0.5155280\n17 Chicago Cubs    1995  73  71 693 671 0.5069444\n18 Chicago Cubs    2005  79  83 703 714 0.4876543\n19 Chicago Cubs    1992  78  84 593 624 0.4814815\n20 Chicago Cubs    1991  77  83 695 734 0.4812500\n21 Chicago Cubs    1985  77  84 686 729 0.4782609\n22 Chicago Cubs    1988  77  85 660 694 0.4753086\n23 Chicago Cubs    1990  77  85 690 774 0.4753086\n24 Chicago Cubs    1987  76  85 720 801 0.4720497\n25 Chicago Cubs    1996  76  86 772 771 0.4691358\n26 Chicago Cubs    2010  75  87 685 767 0.4629630\n27 Chicago Cubs    2022  74  88 657 731 0.4567901\n28 Chicago Cubs    1982  73  89 676 709 0.4506173\n29 Chicago Cubs    2014  73  89 614 707 0.4506173\n30 Chicago Cubs    1983  71  91 701 719 0.4382716\n31 Chicago Cubs    2011  71  91 654 756 0.4382716\n32 Chicago Cubs    2021  71  91 705 839 0.4382716\n33 Chicago Cubs    1986  70  90 680 781 0.4375000\n34 Chicago Cubs    1994  49  64 500 549 0.4336283\n35 Chicago Cubs    1997  68  94 687 759 0.4197531\n36 Chicago Cubs    1999  67  95 747 920 0.4135802\n37 Chicago Cubs    2002  67  95 706 759 0.4135802\n38 Chicago Cubs    2006  66  96 716 834 0.4074074\n39 Chicago Cubs    2013  66  96 602 689 0.4074074\n40 Chicago Cubs    2000  65  97 764 904 0.4012346\n41 Chicago Cubs    1980  64  98 614 728 0.3950617\n42 Chicago Cubs    2012  61 101 613 759 0.3765432\n43 Chicago Cubs    1981  38  65 370 483 0.3689320\n```\n:::\n\n```{.r .cell-code}\n# Compute the mean and standard deviation of winning percentage\ncubs %>% \n  summarize(mean_winning_pct = mean(w_pct),\n            sd_winning_pct = sd(w_pct))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  mean_winning_pct sd_winning_pct\n1        0.4867853     0.06853359\n```\n:::\n:::\n\n\n#### 3. With the full *Teams* data:\n\n-   Compute the `mean` and `standard deviation` of winning percentage for each team\n-   Order by winning percentage, greatest to least\n\n\n::: {.cell}\n\n```{.r .cell-code}\nteams %>% \n  mutate(w_pct = w / (w + l)) %>% # FIRST - compute the winning percentage\n  group_by(name) %>% \n  summarize(n = n(),\n            mean_winning_pct = mean(w_pct, na.rm = TRUE), # compute the average winning percentage for each team\n            sd_winning_pct = sd(w_pct, na.rm = TRUE)) %>% # compute the standard deviation of winning percentage for each team\n  arrange(desc(mean_winning_pct)) # Order by highest winning percentage\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 140 × 4\n   name                         n mean_winning_pct sd_winning_pct\n   <chr>                    <int>            <dbl>          <dbl>\n 1 Boston Red Stockings         5            0.773         0.0911\n 2 Cincinnati Outlaw Reds       1            0.657        NA     \n 3 Boston Reds                  3            0.616         0.0790\n 4 Providence Grays             8            0.609         0.0855\n 5 Chicago White Stockings     17            0.609         0.117 \n 6 Cincinnati Red Stockings     8            0.589         0.0640\n 7 Boston Red Caps              7            0.579         0.0976\n 8 New York Yankees           110            0.577         0.0685\n 9 Brooklyn Ward's Wonders      1            0.576        NA     \n10 Philadelphia Whites          3            0.574         0.0934\n# ℹ 130 more rows\n```\n:::\n:::\n\n\n#### 4. Use the full data to reproduce the plot below\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Please put the code for the plot in this chunk.\nteams %>% \n  as_tibble() %>% \n  mutate(w_pct = w / (w + l)) %>% \n  filter(name == \"New York Yankees\" |\n         name == \"Detroit Tigers\" |\n         name == \"San Diego Padres\") %>% \n  ggplot(aes(year_id, w_pct)) + \n  geom_line(aes(color = name)) +\n  theme_minimal() +\n  labs(\n    color = \"Team\"\n  )\n```\n\n::: {.cell-output-display}\n![](hw4-key_files/figure-html/plot-1.png){width=672}\n:::\n:::\n\n\n# Part 2\n\n#### 1. Read in the following three datasets, using `{here}` and the package of your choice (`{rio}`, `{readr}`, `{haven}`, `{readxl}`)\n\n-   *sesame13.sav*\n-   *star.csv*\n-   *ais.xlsx*\n\n**Hint**: For the *ais.xlsx* data, look at the `skip` argument within the [`{readxl}`](https://readxl.tidyverse.org/reference/read_excel.html) help documentation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsesame <- import(here(\"data\", \"sesame13.sav\"))\nstar <- import(here(\"data\", \"star.csv\"))\nais <- import(here(\"data\", \"ais.xlsx\"), skip = 15)\n```\n:::\n\n\n#### 2. Using the *ais* data, compute the average red blood cell count and average `bmi` by sport. Output these data as SPSS and EXCEL files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmry <- ais %>% \n  group_by(sport) %>% \n  summarize(mean_rcc = mean(rcc),\n            mean_bmi = mean(bmi))\n\nexport(smry, \"summary_measures.sav\")\nexport(smry, \"summary_measures.xlsx\")\n```\n:::\n\n\n#### 3. Use the *sesame* data to answer the following question: Was the average female age higher in schools or at home?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsesame %>% \n  characterize() %>% \n  filter(sex == \"Female\") %>% \n  group_by(setting) %>% \n  summarize(mean_age = mean(age))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  setting mean_age\n  <chr>      <dbl>\n1 Home        49.4\n2 School      53.1\n```\n:::\n\n```{.r .cell-code}\n# Answer: School\n```\n:::\n\n\n#### 4. First, how many rows and columns are in the *star* data? Next, remove outliers using a really poor method, just for practice, by eliminating students whose math (`tmathss`) scores were more than three standard deviations **above *or* below** the corresponding mean. How many rows are in the data now?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(star) # 5748 rows, 12 columns\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5748   12\n```\n:::\n\n```{.r .cell-code}\nstar %>% \n  filter(tmathss <= mean(tmathss) + 3*sd(tmathss) &\n         tmathss >= mean(tmathss) - 3*sd(tmathss)) %>% \n  nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5743\n```\n:::\n\n```{.r .cell-code}\n# Answer: 5743  \n```\n:::\n\n\n#### 5. Use the *star* data to compute standardized math and reading scores; name these variables *tmathss* and *treadss*. To create standardized scores, for each variable (math and reading), subtract the mean from each observation and divide by the standard deviation: $x_s = \\frac{x_i - \\bar{X}}{sd(X)}$.\n\n-   Check that your computations were correct by computing the mean and standard deviation of each variable (they should be 0 and 1, respectively).\n-   Compute the mean of the standardized variable for all `sex`/`frl` combinations. (I'm asking you to extend what you know here. We haven't talked explicitly about how to do this yet, but you have seen examples).\n-   What do you make of these findings? Do you see an effect by `sex`? An `frl` effect (`frl` stands for free/reduced lunch, and is a rough proxy for household income)? Is there evidence of an interaction (i.e., that the effect of `frl` is greater for boys versus girls)?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstar <- star %>% \n  mutate(stand_math = (tmathss - mean(tmathss)) / sd(tmathss),\n         stand_rdg  = (treadss - mean(treadss)) / sd(treadss))\n\nstar %>% \n  summarize(stand_math_mean = mean(stand_math),\n            stand_math_sd   = sd(stand_math),\n            stand_rdg_mean = mean(stand_rdg),\n            stand_rdg_sd   = sd(stand_rdg))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  stand_math_mean stand_math_sd stand_rdg_mean stand_rdg_sd\n1     2.29625e-16             1  -6.278158e-16            1\n```\n:::\n\n```{.r .cell-code}\nstar %>% \n  group_by(sex, frl) %>% \n  summarize(stand_math_mean = mean(stand_math),\n            stand_math_sd   = sd(stand_math),\n            stand_rdg_mean = mean(stand_rdg),\n            stand_rdg_sd   = sd(stand_rdg))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n# Groups:   sex [2]\n  sex   frl   stand_math_mean stand_math_sd stand_rdg_mean stand_rdg_sd\n  <chr> <chr>           <dbl>         <dbl>          <dbl>        <dbl>\n1 boy   no              0.151         0.970          0.148        1.02 \n2 boy   yes            -0.330         0.965         -0.358        0.838\n3 girl  no              0.326         0.962          0.371        1.09 \n4 girl  yes            -0.170         0.969         -0.187        0.863\n```\n:::\n:::\n\n\n#### Please submit you .qmd file, as well as the rendered .html file.\n",
    "supporting": [
      "hw4-key_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}